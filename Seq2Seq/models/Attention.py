#!/usr/bin/python3# -*- coding: utf-8 -*-# @Time    : 2019-09-24 15:29# @Author  : apollo2mars# @File    : Attention.py# @Contact : apollo2mars@gmail.com# @Desc    :import tensorflow as tfclass Attention(object):	def __init__(self, args):		self.args = args		self.input_x = tf.placeholder(tf.int32, [None, self.args.seq_length], name='input_x')		self.input_y = tf.placeholder(tf.float32, [None, self.args.num_classes], name='input_y')		self.global_step = tf.placeholder(shape=(), dtype=tf.int32, name='global_step')		self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')		self.hidden_dim = args.hidden_dim		self.embedding_dim = args.embedding_dim		self.attention_dim = 201		self.rnn()	def _auc_pr(self, true, prob):		depth = self.args.num_classes		pred = tf.one_hot(tf.argmax(prob, -1), depth=depth)		tp = tf.logical_and(tf.cast(pred, tf.bool), tf.cast(true, tf.bool))		acc = tf.truediv(tf.reduce_sum(tf.cast(tp, tf.int32)), tf.shape(pred)[0])		print(acc)		return acc	def self_attention(self):		# embedding lookup		inputs = tf.nn.embedding_lookup(self.embedding_matrix, self.input_x)		# self_attention		with tf.name_scope("self-attention"):			H1 = tf.reshape(inputs, [-1, self.embedding_dim])			W_a1 = tf.get_variable("W_a1", shape=[self.embedding_dim, self.attention_dim], initializer=self.initializer, trainable=True)			u1 = tf.matmul(H1, W_a1)			H2 = tf.reshape(tf.identity(inputs), [-1, self.embedding_dim])			W_a2 = tf.get_variable("W_a2", shape=[self.embedding_dim, self.attention_dim], initializer=self.initializer, trainable=True)			u2 = tf.matmul(H2, W_a2)			u1 = tf.reshape(u1, [self.batch_size, self.seq_len, self.embedding_dim])			u2 = tf.reshape(u2, [self.batch_size, self.seq_len, self.embedding_dim])			u = tf.matmul(u1, u2, transpose_b=True)			# Array of weights for each time step			A = tf.nn.softmax(u, name="attention_matrix")			att_outputs = tf.matmul(A, tf.reshape(tf.identity(inputs), [-1, self.seq_len, self.embedding_dim]))			# linear			# softmax_w = tf.get_variable("softmax_w", [self.embedding_dim, self.class_num], initializer=self.initializer, dtype=tf.float32)			# softmax_b = tf.get_variable("softmax_b", [self.class_num], initializer=self.initializer, dtype=tf.float32)			#			# logits = tf.matmul(tf.reshape(att_outputs, [-1, self.embedding_dim]), softmax_w) + softmax_b			#			# logits = tf.reshape(self.logits, [-1, self.seq_len, self.class_num])		with tf.name_scope("score"):			""" dense layer 1"""			fc = tf.layers.dense(att_outputs, self.args.hidden_dim, name='fc1')			fc = tf.nn.relu(fc)			fc_1 = tf.nn.dropout(fc, self.keep_prob)			""" dense layer 2"""			logits = tf.layers.dense(fc_1, self.args.num_classes, name='fc2')			self.output_softmax = tf.nn.softmax(logits, name='output_softmax')			self.output_argmax = tf.argmax(self.output_softmax, 1, name='output_argmax')  # 预测类别		with tf.name_scope("optimize"):			self.loss = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=self.input_y)			self.learning_rate = tf.train.exponential_decay(learning_rate=self.args.learning_rate,															global_step=self.global_step,															decay_steps=2,															decay_rate=0.95,															staircase=True)			optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)			self.trainer = optimizer.minimize(self.loss)			tf.summary.scalar('loss', self.loss)	def transformer(self):		pass	def soft(self):		pass	def hard(self):		pass