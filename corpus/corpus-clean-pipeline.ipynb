{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36064bit27be6fbe43c041f6a1f58b4ba6c110f6",
   "display_name": "Python 3.6.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'single' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-37ca7bda1177>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 读取各种数据\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m## 单轮\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;34m+\u001b[0m \u001b[0msingle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m小黄鸡\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m微博\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'single' is not defined"
     ]
    }
   ],
   "source": [
    "# 读取各种数据\n",
    "## 单轮 \n",
    "+ single\n",
    "+ 小黄鸡\n",
    "+ 微博\n",
    "\n",
    "\"\"\"\n",
    "你好\n",
    "你好啊\n",
    "\n",
    "天王盖地虎\n",
    "宝塔镇河妖\n",
    "\n",
    "周一日常虚弱\n",
    "周末好好休息才行啊\n",
    "\"\"\"\n",
    "\n",
    "## 多轮\n",
    "+ multi\n",
    "\n",
    "\"\"\"\n",
    "梅西有多高\n",
    "1米六左右吧\n",
    "他和c罗谁踢球更厉害\n",
    "\n",
    "九寨沟在那个省份\n",
    "四川\n",
    "哪里还有其他的好玩的地方吗\n",
    "\"\"\"\n",
    "\n",
    "## 单多轮数据统一处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取多轮数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 默认数据存储\n",
    "all_text_list = []\n",
    "all_conv_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "M\t谢谢你所做的一切\nM\t你开心就好\nM\t开心\nM\t嗯因为你的心里只有学习\nM\t某某某，还有你\nM\t这个某某某用的好\nE\nM\t你们宿舍都是这么厉害的人吗\nM\t眼睛特别搞笑这土也不好捏但就是觉得挺可爱\nM\t特别可爱啊\nE\nM\t今天好点了吗？\nM\t一天比一天严重\nM\t吃药不管用，去打一针。别拖着\nE\nM\t是的。下辈子想做只萤火虫\nM\t可是萤火虫太容易被抓了还是改一个吧\nM\t不，我只想奋不顾身扑火\nE\nM\t加油，三月动起来，五月笑起来\nM\t正解你为什么就那么厉害呢\nM\t哈哈，没办法，智商就是这么高\nM\t你这是要开始得瑟了吗！好啦！你最厉害！\nM\t哈哈哈哈\nE\nM\t好身材，秀出来\nM\t哈哈哈其实我是胖的\nM\t不会的\nE\nM\t因为我网络差吗，加载不出来啊\nM\t是什么网络啊，移动的可能比较慢\nM\t啊真的是我自家网络的问题，用别家的就好了\nE\nM\t这是克拉玛依咩！\nM\t对啊！有没有啥推荐的快快快\nM\t哈哈哈哈哈，要吃东西的话就去那个汉博，那边比较多克拉玛依的凉皮也可以尝尝然后可以去九龙潭看看，克拉玛依图书馆啊，一号井啊都是克拉玛依旅游比较常去的地方\nM\t良心评论\nE\nM\t这个側颜可以\nM\t这个繁体很厉害\n['谢谢你所做的一切', '你开心就好', '开心', '嗯因为你的心里只有学习', '某某某，还有你', '这个某某某用的好']\n['你们宿舍都是这么厉害的人吗', '眼睛特别搞笑这土也不好捏但就是觉得挺可爱', '特别可爱啊']\n['今天好点了吗？', '一天比一天严重', '吃药不管用，去打一针。别拖着']\n['是的。下辈子想做只萤火虫', '可是萤火虫太容易被抓了还是改一个吧', '不，我只想奋不顾身扑火']\n['加油，三月动起来，五月笑起来', '正解你为什么就那么厉害呢', '哈哈，没办法，智商就是这么高', '你这是要开始得瑟了吗！好啦！你最厉害！', '哈哈哈哈']\n['好身材，秀出来', '哈哈哈其实我是胖的', '不会的']\n['因为我网络差吗，加载不出来啊', '是什么网络啊，移动的可能比较慢', '啊真的是我自家网络的问题，用别家的就好了']\n['这是克拉玛依咩！', '对啊！有没有啥推荐的快快快', '哈哈哈哈哈，要吃东西的话就去那个汉博，那边比较多克拉玛依的凉皮也可以尝尝然后可以去九龙潭看看，克拉玛依图书馆啊，一号井啊都是克拉玛依旅游比较常去的地方', '良心评论']\n['这个側颜可以', '这个繁体很厉害', '为了配你的颜', '哥，']\n['我今天腿都废了，你们过节，我搬砖', '辛苦啊，圣诞节还去赚大钱了加油', '毕竟是没男朋友的人，什么节都是一样的']\n"
    }
   ],
   "source": [
    "import os\n",
    "path = '/Users/sunhongchao/Documents/craft/Awesome/Awesome-Corpus/single-multi/multi_round/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "tmp_conv_list = []\n",
    "\n",
    "for file in files:\n",
    "    if not os.path.isdir(file):\n",
    "        f = open(os.path.join(path, file), mode='r', encoding='utf-8')\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip()) < 1:\n",
    "                all_text_list.append('E')\n",
    "                all_conv_list.append(tmp_conv_list.copy())\n",
    "                tmp_conv_list = []\n",
    "            else:\n",
    "                all_text_list.append('M' + '\\t' + line.strip())\n",
    "                tmp_conv_list.append(line.strip())\n",
    "\n",
    "for item in all_text_list[:40]:\n",
    "    print(item)\n",
    "\n",
    "for item in all_conv_list[:10]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 排序去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4999154\n0.9998308\n[['!\"\"\"这个夜晚的印记\"!', '?!.!', '!,!.'], [\"!'!\", \"!'.\", \"!.'\"], [\"!'!\", '你不是吗？', '我是', '这不是你给我找的西语名吗'], [\"!'!\", '感谢喜欢', '画得真棒'], [\"!'!\", '说人话', '我用的是微博国际版'], [\"!'!\", '这样就对了，正常聊天多好，哈', '什么叫正常聊天啊？我那不是吗！'], [\"!'!\", '需要用过去式谢谢', '我乐意！'], [\"!'\", '我的微博简介叼不叼', '开心就好'], [\"!''.纯粹的幸福！捕捉那些你不可能想要忘记的时刻。上海迪士尼童话乐园爱是一种信仰把你带回我的身旁每个人都是孩子永远比明天小生活因喜悦而精彩!喜悦因分享而永恒!\", '调皮', '我要等到月底才能去找你了过几天去趟深圳', '噢你也是很浪'], [\"!'.!\", \".'.\", \".!.'!'\", \".''.\"]]\n"
    }
   ],
   "source": [
    "all_conv_str_list = list(set([\"$$$\".join(item) for item in all_conv_list]))\n",
    "print(len(all_conv_str_list))\n",
    "print(len(all_conv_str_list)/len(all_conv_list))\n",
    "sorted_all_conv_str_list = sorted(all_conv_str_list)\n",
    "# all_conv_list = [ item.split(\"$$$\") for item in sorted_all_conv_str_list]\n",
    "# print(all_conv_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 繁体转简体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "tmp_list = []\n",
    "for item  in all_conv_str_list:\n",
    "    tmp_list.append(SnowNLP(item).han)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 拼音转汉字"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 英文转中文"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycorrector"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 话术是否连贯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句法分析 == >>> 不满足特定条件的句法树 删除"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 无有效中文"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 长句压缩\n",
    "+ 句子过长，进行压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "\n",
    "tmp_list = []\n",
    "for item in tmp_list:\n",
    "    s = SnowNLP(item)\n",
    "    s.summary(limit=4) # limit 为最长多少句"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 噪声过滤 - 包含即删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 标点符号\n",
    "punc_list = ['#','#', '-','<','>','《','》', \"@\", '【', '】', '？', ' ——', '_','_', '\"', \"'\",':','：', '‘','’']\n",
    "# 噪声过滤\n",
    "ad_list = ['直播','庆祝','销量', '销量','到货']\n",
    "news_list = ['事件']\n",
    "dirty_list = ['撸']\n",
    "web_list = ['写出你', '转一个','转运', '达人秀','演唱会','明星', '图片', '文章', 'mv', '视频', '传说中','吃吃吃','礼物','朱艳艳','做客','记录','Nick','中国梦','博鳌','求救']\n",
    "\n",
    "reference_list = ['这个', '那个', '哪个']\n",
    "politics_list = ['下台','政绩工程', '毛泽东']\n",
    "special_list=[ 'si','鸡鸡','小通','旺财']\n",
    "security_list = ['查询', '搞蒙', '你这句话']\n",
    "\n",
    "locations_list = ['成都', '北京', '上海']\n",
    "greetings_list = ['晚安', '早安', '上午好','下午好','晚上好']\n",
    "time_list = ['早上','清早','凌晨','上午','中午','晚上','今晚','昨天','明天','后天']\n",
    "weather_lsit = ['PM', '北京空气']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 噪声过滤 - 部分剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "match_list = ['转起来'， '分享自', '链接','转', ]\n",
    "\n",
    "#那一年，你还记得你床头的第一张明星的海报，是为谁而贴？转\n",
    "\n",
    "def replace_list(input_list:list):\n",
    "    tmp_list = []\n",
    "\n",
    "    for item in input_list:\n",
    "        text_re = re.compile('【.*】')\n",
    "        item = text_re.sub('', item)\n",
    "\n",
    "        text_re = re.compile('#.*#')\n",
    "        item = text_re.sub('', item)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类器 剔除 不属于闲聊场景话术"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 大促\n",
    "+ 多话术 300\n",
    "+ 扩展后  1400\n",
    "# IOT\n",
    "+ 数量少，质量差\n",
    "\n",
    "# 半监督文本分类\n",
    "+ 其他分类器 来 分当前话术 \n",
    "+ 清华 数据 不包括闲聊数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "[0, 0, 0, 0, 1.1364954710359758]\n"
    }
   ],
   "source": [
    "from snownlp import SnowNLP\n",
    "lines = ['今天科大讯飞的股票涨了吗', '新年第一天北京下起了大雪', '山东舰今天在三亚服役了', '五一小长假去西藏玩', '开战了，美国打伊朗了']\n",
    "s = SnowNLP(lines)\n",
    "print(s.sim('温度很低，注意保暖'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build testfile.csv\n",
    "import csv\n",
    "#打开文件，追加a\n",
    "out = open('/Users/sunhongchao/Documents/craft/tmp.csv','a', newline='')\n",
    "#设定写入模式\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "#写入具体内容\n",
    "pre_text = [\"id_left\", \"text_left\", \"id_right\", \"text_right\", \"label\"]\n",
    "csv_write.writerow(pre_text)\n",
    "all_conv_list = [['aaa', 12], ['ccc', 33]]\n",
    "for idx, item in enumerate(all_conv_list):\n",
    "    tmp_list = [0, 'Q{}'.format(idx), item[0], 'D{}-1'.format(idx), item[1], '1.0']\n",
    "    csv_write.writerow(tmp_list)\n",
    "\n",
    "\n",
    "print (\"write over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ,id_left,text_left,id_right,text_right,label\n",
    "# 0,Q19,你老板是谁 ,D19-1,我老板qq：929788643,1.0\n",
    "# 1,Q19,你老板是谁 ,D19-2,一天狂撸1w次，而且射100w次,0.0\n",
    "# 2,Q19,你老板是谁 ,D19-3,我不抠,0.0\n",
    "# 3,Q19,你老板是谁 ,D19-4,你也玩彩票？这玩意一不小心就能发大财,0.0\n",
    "# 4,Q19,你老板是谁 ,D19-5,哦哦...,0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2.1.0\n  id_left text_left id_right          text_right  label\n0     Q19    你老板是谁     D19-1     我老板qq：929788643    1.0\n1     Q19    你老板是谁     D19-2    一天狂撸1w次，而且射100w次    0.0\n2     Q19    你老板是谁     D19-3                 我不抠    0.0\n3     Q19    你老板是谁     D19-4  你也玩彩票？这玩意一不小心就能发大财    0.0\n4     Q19    你老板是谁     D19-5               哦哦...    0.0\n"
    }
   ],
   "source": [
    "import matchzoo as mz\n",
    "import pandas as pd\n",
    "print(mz.__version__)\n",
    "data_pack = mz.pack(pd.read_csv('/Users/sunhongchao/Documents/craft/testfile.csv', index_col=0))\n",
    "data_pack.relation['label'] = data_pack.relation['label'].astype('float32')\n",
    "frame = data_pack.frame\n",
    "print(frame().head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "Processing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter: 100%|██████████| 2118/2118 [00:00<00:00, 6474.00it/s]\nProcessing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter: 100%|██████████| 18841/18841 [00:05<00:00, 3172.04it/s]\nProcessing text_left with extend: 100%|██████████| 2118/2118 [00:00<00:00, 451490.95it/s]\nProcessing text_right with extend: 100%|██████████| 18841/18841 [00:00<00:00, 319853.33it/s]\nBuilding Vocabulary from a datapack.: 100%|██████████| 1614998/1614998 [00:00<00:00, 2733175.39it/s]\nProcessing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter => WordHashing: 100%|██████████| 2118/2118 [00:01<00:00, 1671.68it/s]\nProcessing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter => WordHashing: 100%|██████████| 18841/18841 [00:18<00:00, 1028.92it/s]\nProcessing text_left with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter => WordHashing: 100%|██████████| 296/296 [00:00<00:00, 1098.63it/s]\nProcessing text_right with chain_transform of Tokenize => Lowercase => PuncRemoval => StopRemoval => NgramLetter => WordHashing: 100%|██████████| 2708/2708 [00:02<00:00, 1109.24it/s]\nWARNING: PairDataGenerator will be deprecated in MatchZoo v2.2. Use `DataGenerator` with callbacks instead.\nEpoch 1/20\n 8/16 [==============>...............] - ETA: 5s - loss: 1.6049/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.130172). Check your callbacks.\n  % delta_t_median)\n/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.126865). Check your callbacks.\n  % delta_t_median)\n16/16 [==============================] - 6s 400ms/step - loss: 1.5968\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.20417550327340023 - mean_average_precision(0.0): 0.21111618611618613\nEpoch 2/20\n16/16 [==============================] - 2s 155ms/step - loss: 1.5158\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2011265946383195 - mean_average_precision(0.0): 0.2163703038703039\nEpoch 3/20\n 8/16 [==============>...............] - ETA: 1s - loss: 1.4798/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.127609). Check your callbacks.\n  % delta_t_median)\n/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.125894). Check your callbacks.\n  % delta_t_median)\n16/16 [==============================] - 2s 156ms/step - loss: 1.4729\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.22403901825853248 - mean_average_precision(0.0): 0.2327045795795796\nEpoch 4/20\n16/16 [==============================] - 2s 153ms/step - loss: 1.4387\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2246324096193814 - mean_average_precision(0.0): 0.23320508008008012\nEpoch 5/20\n16/16 [==============================] - 2s 156ms/step - loss: 1.4129\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.24867242425608346 - mean_average_precision(0.0): 0.2506190168690169\nEpoch 6/20\n16/16 [==============================] - 2s 154ms/step - loss: 1.3948\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2574005273798446 - mean_average_precision(0.0): 0.25614394426894427\nEpoch 7/20\n 6/16 [==========>...................] - ETA: 2s - loss: 1.3828/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.125506). Check your callbacks.\n  % delta_t_median)\n16/16 [==============================] - 3s 160ms/step - loss: 1.3758\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2706519393016234 - mean_average_precision(0.0): 0.2681688384813385\nEpoch 8/20\n16/16 [==============================] - 3s 160ms/step - loss: 1.3612\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.26719342911822536 - mean_average_precision(0.0): 0.26962676962676957\nEpoch 9/20\n16/16 [==============================] - 3s 161ms/step - loss: 1.3465\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2604366723614686 - mean_average_precision(0.0): 0.2561422755172755\nEpoch 10/20\n 8/16 [==============>...............] - ETA: 1s - loss: 1.3382/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.126942). Check your callbacks.\n  % delta_t_median)\n/Users/sunhongchao/anaconda3/envs/tf_base/lib/python3.6/site-packages/keras/callbacks.py:122: UserWarning: Method on_batch_end() is slow compared to the batch update (0.126692). Check your callbacks.\n  % delta_t_median)\n16/16 [==============================] - 2s 153ms/step - loss: 1.3369\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2622660695746167 - mean_average_precision(0.0): 0.2582363338613339\nEpoch 11/20\n16/16 [==============================] - 2s 156ms/step - loss: 1.3195\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.24634982307318673 - mean_average_precision(0.0): 0.24992883805383803\nEpoch 12/20\n16/16 [==============================] - 2s 156ms/step - loss: 1.3061\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2603565405564489 - mean_average_precision(0.0): 0.263632683007683\nEpoch 13/20\n16/16 [==============================] - 2s 152ms/step - loss: 1.2955\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2682126562972599 - mean_average_precision(0.0): 0.26360735735735735\nEpoch 14/20\n16/16 [==============================] - 3s 157ms/step - loss: 1.2819\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2766786578292863 - mean_average_precision(0.0): 0.2722678035178035\nEpoch 15/20\n16/16 [==============================] - 2s 156ms/step - loss: 1.2755\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.24713785733490487 - mean_average_precision(0.0): 0.2525258619008619\nEpoch 16/20\n16/16 [==============================] - 3s 174ms/step - loss: 1.2611\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2314127831179326 - mean_average_precision(0.0): 0.2395203238953239\nEpoch 17/20\n16/16 [==============================] - 2s 156ms/step - loss: 1.2489\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.238424389074747 - mean_average_precision(0.0): 0.2408086658086658\nEpoch 18/20\n16/16 [==============================] - 3s 161ms/step - loss: 1.2335\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.24500273720050272 - mean_average_precision(0.0): 0.24394841269841272\nEpoch 19/20\n16/16 [==============================] - 2s 153ms/step - loss: 1.2272\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2663451885573188 - mean_average_precision(0.0): 0.26638659763659767\nEpoch 20/20\n16/16 [==============================] - 2s 155ms/step - loss: 1.2139\nValidation: normalized_discounted_cumulative_gain@3(0.0): 0.2637421713265043 - mean_average_precision(0.0): 0.2661895255645256\n"
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download()\n",
    "import matchzoo as mz\n",
    "\n",
    "# 加载数据\n",
    "train_pack = mz.datasets.wiki_qa.load_data('train', task='ranking')\n",
    "valid_pack = mz.datasets.wiki_qa.load_data('dev', task='ranking')\n",
    "# 加载 DSSM 模型\n",
    "preprocessor = mz.preprocessors.DSSMPreprocessor()\n",
    "# 处理数据\n",
    "train_processed = preprocessor.fit_transform(train_pack)\n",
    "valid_processed = preprocessor.transform(valid_pack)\n",
    "# 排序指标\n",
    "ranking_task = mz.tasks.Ranking(loss=mz.losses.RankCrossEntropyLoss(num_neg=4)) # 负样本四个\n",
    "ranking_task.metrics = [\n",
    "    mz.metrics.NormalizedDiscountedCumulativeGain(k=3), # ndcg\n",
    "    mz.metrics.MeanAveragePrecision() # map\n",
    "]\n",
    "# 模型构建 编译\n",
    "model = mz.models.DSSM()\n",
    "model.params['input_shapes'] = preprocessor.context['input_shapes']\n",
    "model.params['task'] = ranking_task\n",
    "model.guess_and_fill_missing_params()\n",
    "model.build()\n",
    "model.compile()\n",
    "# 生成训练数据， 验证数据\n",
    "train_generator = mz.PairDataGenerator(train_processed, num_dup=1, num_neg=4, batch_size=64, shuffle=True)\n",
    "valid_x, valid_y = valid_processed.unpack()\n",
    "# 进行验证\n",
    "evaluate = mz.callbacks.EvaluateAllMetrics(model, x=valid_x, y=valid_y, batch_size=len(valid_x))\n",
    "# 训练模型\n",
    "history = model.fit_generator(train_generator, epochs=20, callbacks=[evaluate], workers=5, use_multiprocessing=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "2.1.0\nRanking Task\n"
    }
   ],
   "source": [
    "import matchzoo as mz\n",
    "print(mz.__version__)\n",
    "\n",
    "### 定义任务，包含两种，一个是Ranking，一个是classification\n",
    "task = mz.tasks.Ranking()\n",
    "print(task)\n",
    "\n",
    "### 准备数据，数据在源码中有，不确定在pip安装的是否存在\n",
    "### train_raw是matchzoo中自定的数据格式\tmatchzoo.data_pack.data_pack.DataPack\n",
    "train_raw = mz.datasets.toy.load_data(stage='train', task=task)\n",
    "test_raw = mz.datasets.toy.load_data(stage='test', task=task)\n",
    "\n",
    "### 数据预处理，BasicPreprocessor为指定预处理的方式，在预处理中包含了两步：fit,transform\n",
    "### fit将收集一些有用的信息到preprocessor.context中，不会对输入DataPack进行处理\n",
    "### transformer 不会改变context、DataPack,他将重新生成转变后的DataPack.\n",
    "### 在transformer过程中，包含了Tokenize => Lowercase => PuncRemoval等过程，这个过程在方法中应该是可以自定义的\n",
    "preprocessor = mz.preprocessors.BasicPreprocessor()\n",
    "preprocessor.fit(train_raw)  ## init preprocessor inner state.\n",
    "train_processed = preprocessor.transform(train_raw)\n",
    "test_processed = preprocessor.transform(test_raw)\n",
    "\n",
    "### 创建模型以及修改参数（可以使用mz.models.list_available()查看可用的模型列表）\n",
    "model = mz.models.DenseBaseline()\n",
    "model.params['task'] = task\n",
    "model.params['mlp_num_units'] = 3\n",
    "model.params.update(preprocessor.context)\n",
    "model.params.completed()\n",
    "model.build()\n",
    "model.compile()\n",
    "model.backend.summary()\n",
    "\n",
    "### 训练, 评估, 预测\n",
    "x, y = train_processed.unpack()\n",
    "test_x, test_y = test_processed.unpack()\n",
    "model.fit(x , y,batch_size=32, epochs=5)\n",
    "model.evaluate(test_x,test_y)\n",
    "model.predict(test_x)\n",
    "\n",
    "### 保存模型\n",
    "model.save('my-model')\n",
    "loaded_model = mz.load_model('my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/sunhongchao/Documents/craft/Awesome-Corpus/done/multi-answer-pure.txt\", mode='w', encoding='utf-8') as f:\n",
    "    f.writelines(tmp_answer_list)\n",
    "\n",
    "with open(\"/Users/sunhongchao/Documents/craft/Awesome-Corpus/done/multi-question-pure.txt\", mode='w', encoding='utf-8') as f:\n",
    "    f.writelines(tmp_query_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 获取字典  word2idx\n",
    "vocab = open('/export/home/sunhongchao1/Prototype-Robot/solutions/FAQ/NLG/seqGAN/gen_data/vocab5000.all', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "word2idx = {}\n",
    "\n",
    "for idx, value in enumerate(vocab):\n",
    "    word2idx[value.strip()] = idx\n",
    "\n",
    "print(word2idx)\n",
    "\n",
    "# 获取 embedding matrix\n",
    "embedding_path = '/export/home/sunhongchao1/Workspace-of-NLU/resources/Tencent_AILab_ChineseEmbedding.txt'\n",
    "\n",
    "fin = open(embedding_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "word2vec = {}\n",
    "for line in fin:\n",
    "    tokens = line.rstrip().split(' ') \n",
    "    if tokens[0] in word2idx.keys():\n",
    "        word2vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx), 200))\n",
    "unknown_words_vector = np.random.rand(200)\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in word2vec.keys():\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = unknown_words_vector\n",
    "\n",
    "\n",
    "history, true_utt, false_utt = [], [], []\n",
    "\n",
    "\"\"\"\n",
    "读取单轮数据\n",
    "\"\"\"\n",
    "with open(\"/export/home/sunhongchao1/Prototype-Robot/corpus/dialogue/new_corpus.txt\", mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for idx in range(0, len(lines)-3, 3):\n",
    "        history.append([ [ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in lines[idx][2:]]]) # 加入的是一个list\n",
    "        true_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in lines[idx+1][2:]])\n",
    "        tmp_utt =lines[random.choice(range(len(lines)//3)) + 1 ][2:]\n",
    "        false_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_utt ])\n",
    "\n",
    "\"\"\"\n",
    "读取多轮数据\n",
    "默认一问一答\n",
    "\"\"\"\n",
    "\n",
    "def random_choice_false_response(lines):\n",
    "\n",
    "    tmp_flag = True\n",
    "    while tmp_flag:\n",
    "        idx = random.choice(len(lines))\n",
    "        \n",
    "        if lines[idx].startwith('M'):\n",
    "            tmp_flag = False\n",
    "\n",
    "    return lines[idx]\n",
    "\n",
    "with open(\"/export/home/sunhongchao1/Prototype-Robot/corpus/dialogue/new_corpus_multi.txt\", mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    tmp_text_list = []\n",
    "    for idx in range(lines):\n",
    "        if lines[idx].startwith('E'):\n",
    "            tmp_history = []\n",
    "            for item in tmp_text_list[:-1]:\n",
    "                tmp_history.append([word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in item])    \n",
    "            history.append(tmp_history)\n",
    "\n",
    "            true_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_test_list[-1]])\n",
    "\n",
    "            tmp_utt = random_choice_false_response(lines)\n",
    "            false_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_utt ])\n",
    "            tmp_text_list = []\n",
    "        elif lines[idx].startwith('M'):\n",
    "            tmp_text_list.append(lines[idx][2:])\n",
    "        else:\n",
    "            print('not start with E or M error')\n",
    "\n",
    "import pickle\n",
    "results = {'history':history, 'true_utt':true_utt, 'false_utt':false_utt}\n",
    "save_file = open(\"results.pkl\",\"wb\")\n",
    "pickle.dump(results, save_file)\n",
    "save_file.close()\n",
    "\n",
    "embedding = {'embedding_matrix':embedding_matrix}\n",
    "save_file = open(\"embedding_matrix.pkl\",\"wb\")\n",
    "pickle.dump(embedding, save_file)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判断是否是汉字\n",
    "def is_all_chinese(strs):\n",
    "    for _char in strs.strip():\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_chinese(uchar)：\n",
    "    if uchar >= u'\\u4E00' and uchar <= u'\\u9FA5':\n",
    "        return true\n",
    "    else:\n",
    "        return false\n",
    "\n",
    "#判断是否是数字\n",
    "def is_number(uchar)：\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return true\n",
    "    else:\n",
    "        return false\n",
    "\n",
    "#判断是否是英文字母\n",
    "def is_english(uchar)：\n",
    "    if ( uchar >= u'\\u0041' and uchar <= u'\\u005A' ) or ( uchar >= u'\\u0061' and uchar <= u'\\u007A'):\n",
    "        return true\n",
    "    else:\n",
    "        return false"
   ]
  }
 ]
}