{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36064bit27be6fbe43c041f6a1f58b4ba6c110f6",
   "display_name": "Python 3.6.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取各种数据\n",
    "## 单轮 \n",
    "+ single\n",
    "+ 小黄鸡\n",
    "+ 微博\n",
    "\n",
    "\"\"\"\n",
    "你好\n",
    "你好啊\n",
    "\n",
    "天王盖地虎\n",
    "宝塔镇河妖\n",
    "\n",
    "周一日常虚弱\n",
    "周末好好休息才行啊\n",
    "\"\"\"\n",
    "\n",
    "## 多轮\n",
    "+ multi\n",
    "\n",
    "\"\"\"\n",
    "梅西有多高\n",
    "1米六左右吧\n",
    "他和c罗谁踢球更厉害\n",
    "\n",
    "九寨沟在那个省份\n",
    "四川\n",
    "哪里还有其他的好玩的地方吗\n",
    "\"\"\"\n",
    "\n",
    "## 单多轮数据统一处理\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 读取多轮数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 默认数据存储\n",
    "all_text_list = []\n",
    "all_conv_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "M\t谢谢你所做的一切\nM\t你开心就好\nM\t开心\nM\t嗯因为你的心里只有学习\nM\t某某某，还有你\nM\t这个某某某用的好\nE\nM\t你们宿舍都是这么厉害的人吗\nM\t眼睛特别搞笑这土也不好捏但就是觉得挺可爱\nM\t特别可爱啊\n['谢谢你所做的一切', '你开心就好', '开心', '嗯因为你的心里只有学习', '某某某，还有你', '这个某某某用的好']\n['你们宿舍都是这么厉害的人吗', '眼睛特别搞笑这土也不好捏但就是觉得挺可爱', '特别可爱啊']\n['今天好点了吗？', '一天比一天严重', '吃药不管用，去打一针。别拖着']\n['是的。下辈子想做只萤火虫', '可是萤火虫太容易被抓了还是改一个吧', '不，我只想奋不顾身扑火']\n['加油，三月动起来，五月笑起来', '正解你为什么就那么厉害呢', '哈哈，没办法，智商就是这么高', '你这是要开始得瑟了吗！好啦！你最厉害！', '哈哈哈哈']\n"
    }
   ],
   "source": [
    "import os\n",
    "path = '/Users/sunhongchao/Documents/craft/Awesome/Awesome-Corpus/single-multi/multi_round/'\n",
    "files = os.listdir(path)\n",
    "\n",
    "tmp_conv_list = []\n",
    "\n",
    "for file in files:\n",
    "    if not os.path.isdir(file):\n",
    "        f = open(os.path.join(path, file), mode='r', encoding='utf-8')\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            if len(line.strip()) < 1:\n",
    "                all_text_list.append('E')\n",
    "                all_conv_list.append(tmp_conv_list.copy())\n",
    "                tmp_conv_list = []\n",
    "            else:\n",
    "                all_text_list.append('M' + '\\t' + line.strip())\n",
    "                tmp_conv_list.append(line.strip())\n",
    "\n",
    "for item in all_text_list[:10]:\n",
    "    print(item)\n",
    "\n",
    "for item in all_conv_list[:5]:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 排序去重"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4999154\n0.9998308\n"
    }
   ],
   "source": [
    "all_conv_str_list = list(set([\"$$$\".join(item) for item in all_conv_list]))\n",
    "print(len(all_conv_str_list))\n",
    "print(len(all_conv_str_list)/len(all_conv_list))\n",
    "sorted_all_conv_str_list = sorted(all_conv_str_list)\n",
    "# all_conv_list = [ item.split(\"$$$\") for item in sorted_all_conv_str_list]\n",
    "# print(all_conv_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 繁体转简体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "tmp_list = []\n",
    "for item  in all_conv_str_list:\n",
    "    tmp_list.append(SnowNLP(item).han)\n",
    "all_conv_str_list = tmp_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_conv_str_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 拼音转汉字"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 英文转中文"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 纠错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pycorrector"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 反问/疑问"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 话术是否连贯"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 句法分析 == >>> 不满足特定条件的句法树 删除"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 无有效中文"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预处理 - 长句压缩\n",
    "+ 句子过长，进行压缩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from snownlp import SnowNLP\n",
    "\n",
    "# tmp_list = []\n",
    "# for item in tmp_list:\n",
    "#     s = SnowNLP(item)\n",
    "#     s.summary(limit=4) # limit 为最长多少句"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 噪声过滤 - 包含即删除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "4999154\n3022144\n"
    }
   ],
   "source": [
    "# 标点符号\n",
    "punc_list = ['#','#', '-','<','>','《','》', \"@\", '【', '】', '？', ' ——', '_','_', '\"', \"'\",':','：', '‘','’',  '°C', 'C°']\n",
    "# 噪声过滤\n",
    "ad_list = ['直播','庆祝','销量', '销量','到货']\n",
    "news_list = ['事件']\n",
    "dirty_list = ['撸', '鸡巴', '屁']\n",
    "web_list = ['写出你', '转一个','转运', '达人秀','演唱会','明星', '图片', '文章', 'mv', '视频', '传说中','吃吃吃','礼物','朱艳艳','做客','记录','Nick','中国梦','博鳌','求救']\n",
    "\n",
    "reference_list = ['这个', '那个', '哪个']\n",
    "politics_list = ['下台','政绩工程', '毛泽东']\n",
    "special_list=[ 'si','鸡鸡','小通','旺财']\n",
    "security_list = ['查询', '搞蒙', '你这句话']\n",
    "\n",
    "locations_list = ['成都', '北京', '上海']\n",
    "greetings_list = ['晚安', '早安', '上午好','下午好','晚上好']\n",
    "time_list = ['早上','清早','凌晨','上午','中午','晚上','今晚','昨天','明天','后天']\n",
    "weather_lsit = ['PM', '北京空气']\n",
    "\n",
    "check_list = punc_list + ad_list + news_list + dirty_list + web_list + reference_list + politics_list + special_list + security_list + locations_list + greetings_list + time_list + weather_lsit\n",
    "\n",
    "def check_error(input_text):\n",
    "    for item in check_list:\n",
    "        if item in input_text:\n",
    "            return True\n",
    "    return  False\n",
    "\n",
    "tmp_list = []\n",
    "\n",
    "for line in all_conv_str_list:\n",
    "    if check_error(line) is False:\n",
    "        tmp_list.append(line)\n",
    "\n",
    "print(len(all_conv_str_list))\n",
    "all_conv_str_list = tmp_list \n",
    "print(len(all_conv_str_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 噪声过滤 - 部分剔除"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "match_list = ['转起来', '分享自', '链接','转']\n",
    "\n",
    "#那一年，你还记得你床头的第一张明星的海报，是为谁而贴？转\n",
    "\n",
    "def replace_list(input_list:list):\n",
    "    tmp_list = []\n",
    "\n",
    "    for item in input_list:\n",
    "        text_re = re.compile('【.*】')\n",
    "        item = text_re.sub('', item)\n",
    "        text_re = re.compile('#.*#')\n",
    "        item = text_re.sub('', item)\n",
    "\n",
    "        for tmp in match_list:\n",
    "            item = item.replace(tmp, '')\n",
    "\n",
    "        tmp_list.append(item)\n",
    "        \n",
    "    return tmp_list\n",
    "\n",
    "all_conv_str_list = replace_list(all_conv_str_list)\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_conv_str_list[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_conv_list = [ item.split(\"$$$\") for item in all_conv_str_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_conv_list[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分类器 剔除 不属于闲聊场景话术"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 大促\n",
    "+ 多话术 300\n",
    "+ 扩展后  1400\n",
    "#### IOT\n",
    "+ 数量少，质量差\n",
    "\n",
    "#### 半监督文本分类\n",
    "+ 其他分类器 来 分当前话术 \n",
    "+ 清华 数据 不包括闲聊数据\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# QA Match"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## snownlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snownlp import SnowNLP\n",
    "lines = ['今天科大讯飞的股票涨了吗', '新年第一天北京下起了大雪', '山东舰今天在三亚服役了', '五一小长假去西藏玩', '开战了，美国打伊朗了']\n",
    "s = SnowNLP(lines)\n",
    "print(s.sim('温度很低，注意保暖'))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## matchzoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "write over\n"
    }
   ],
   "source": [
    "# 构造数据\n",
    "import csv,random\n",
    "import numpy as np\n",
    "out = open('/Users/sunhongchao/Documents/craft/match-zoo-corpus.csv',mode='w', encoding='utf-8', newline='')\n",
    "csv_write = csv.writer(out,dialect='excel')\n",
    "\n",
    "pre_text = [\"text_left\", \"text_right\", \"label\"]\n",
    "csv_write.writerow(pre_text)\n",
    "for item in all_conv_list:\n",
    "    csv_write.writerow([str(item[0]), str(item[1]), '1'])\n",
    "    count = 0\n",
    "    while count < 5:\n",
    "        tmp = random.choice(all_conv_list)\n",
    "        if len(tmp[1].strip()) > 3:\n",
    "            csv_write.writerow([str(item[0]), str(tmp[1]), '0']) # 改进方式，不适用随机文本，改为使用 AA 相似文本\n",
    "            count = count + 1\n",
    "\n",
    "print (\"write over\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读数据\n",
    "import matchzoo as mz\n",
    "import pandas as pd\n",
    "print(mz.__version__)\n",
    "data_pack = mz.pack(pd.read_csv('/Users/sunhongchao/Documents/craft/match-zoo-corpus.csv')) # 必须有 text_left, text_right, label, 其他的会自动补齐\n",
    "data_pack.relation['label'] = data_pack.relation['label'].astype('float32')\n",
    "frame = data_pack.frame\n",
    "print(frame().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(frame().head(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matchzoo as mz\n",
    "\n",
    "### 定义任务，包含两种，一个是Ranking，一个是classification\n",
    "task = mz.tasks.Ranking()\n",
    "### 准备数据，数据在源码中有，不确定在pip安装的是否存在\n",
    "### train_raw是matchzoo中自定的数据格式\tmatchzoo.data_pack.data_pack.DataPack\n",
    "train_raw = data_pack # mz.datasets.toy.load_data(stage='train', task=task)\n",
    "test_raw = data_pack # mz.datasets.toy.load_data(stage='test', task=task)\n",
    "\n",
    "### 数据预处理，BasicPreprocessor为指定预处理的方式，在预处理中包含了两步：fit,transform\n",
    "### fit将收集一些有用的信息到preprocessor.context中，不会对输入DataPack进行处理\n",
    "### transformer 不会改变context、DataPack,他将重新生成转变后的DataPack.\n",
    "### 在transformer过程中，包含了Tokenize => Lowercase => PuncRemoval等过程，这个过程在方法中应该是可以自定义的\n",
    "# preprocessor = mz.preprocessors.BasicPreprocessor()\n",
    "preprocessor = mz.preprocessors.DSSMPreprocessor()\n",
    "preprocessor.fit(train_raw, verbose=5)  ## init preprocessor inner state.\n",
    "train_processed = preprocessor.transform(train_raw, verbose=5)\n",
    "test_processed = preprocessor.transform(test_raw, verbose=5)\n",
    "\n",
    "### 创建模型以及修改参数（可以使用mz.models.list_available()查看可用的模型列表）\n",
    "model = mz.models.DenseBaseline()\n",
    "model.params['task'] = task\n",
    "model.params['mlp_num_units'] = 3\n",
    "model.params.update(preprocessor.context)\n",
    "model.params.completed()\n",
    "model.build()\n",
    "model.compile()\n",
    "model.backend.summary()\n",
    "\n",
    "### 训练, 评估, 预测\n",
    "x, y = train_processed.unpack()\n",
    "test_x, test_y = test_processed.unpack()\n",
    "model.fit(x , y,batch_size=32, epochs=5)\n",
    "model.evaluate(test_x,test_y)\n",
    "model.predict(test_x)\n",
    "\n",
    "### 保存模型\n",
    "model.save('my-model')\n",
    "loaded_model = mz.load_model('my-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"/Users/sunhongchao/Documents/craft/Awesome-Corpus/done/multi-answer-pure.txt\", mode='w', encoding='utf-8') as f:\n",
    "    f.writelines(tmp_answer_list)\n",
    "\n",
    "with open(\"/Users/sunhongchao/Documents/craft/Awesome-Corpus/done/multi-question-pure.txt\", mode='w', encoding='utf-8') as f:\n",
    "    f.writelines(tmp_query_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# 获取字典  word2idx\n",
    "vocab = open('/export/home/sunhongchao1/Prototype-Robot/solutions/FAQ/NLG/seqGAN/gen_data/vocab5000.all', 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "word2idx = {}\n",
    "\n",
    "for idx, value in enumerate(vocab):\n",
    "    word2idx[value.strip()] = idx\n",
    "\n",
    "print(word2idx)\n",
    "\n",
    "# 获取 embedding matrix\n",
    "embedding_path = '/export/home/sunhongchao1/Workspace-of-NLU/resources/Tencent_AILab_ChineseEmbedding.txt'\n",
    "\n",
    "fin = open(embedding_path, 'r', encoding='utf-8', newline='\\n', errors='ignore')\n",
    "word2vec = {}\n",
    "for line in fin:\n",
    "    tokens = line.rstrip().split(' ') \n",
    "    if tokens[0] in word2idx.keys():\n",
    "        word2vec[tokens[0]] = np.asarray(tokens[1:], dtype='float32')\n",
    "\n",
    "embedding_matrix = np.zeros((len(word2idx), 200))\n",
    "unknown_words_vector = np.random.rand(200)\n",
    "\n",
    "for word, idx in word2idx.items():\n",
    "    if word in word2vec.keys():\n",
    "        embedding_matrix[idx] = word2vec[word]\n",
    "    else:\n",
    "        embedding_matrix[idx] = unknown_words_vector\n",
    "\n",
    "\n",
    "history, true_utt, false_utt = [], [], []\n",
    "\n",
    "\"\"\"\n",
    "读取单轮数据\n",
    "\"\"\"\n",
    "with open(\"/export/home/sunhongchao1/Prototype-Robot/corpus/dialogue/new_corpus.txt\", mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    for idx in range(0, len(lines)-3, 3):\n",
    "        history.append([ [ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in lines[idx][2:]]]) # 加入的是一个list\n",
    "        true_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in lines[idx+1][2:]])\n",
    "        tmp_utt =lines[random.choice(range(len(lines)//3)) + 1 ][2:]\n",
    "        false_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_utt ])\n",
    "\n",
    "\"\"\"\n",
    "读取多轮数据\n",
    "默认一问一答\n",
    "\"\"\"\n",
    "\n",
    "def random_choice_false_response(lines):\n",
    "\n",
    "    tmp_flag = True\n",
    "    while tmp_flag:\n",
    "        idx = random.choice(len(lines))\n",
    "        \n",
    "        if lines[idx].startwith('M'):\n",
    "            tmp_flag = False\n",
    "\n",
    "    return lines[idx]\n",
    "\n",
    "with open(\"/export/home/sunhongchao1/Prototype-Robot/corpus/dialogue/new_corpus_multi.txt\", mode='r', encoding='utf-8') as f:\n",
    "    lines = f.readlines()\n",
    "    tmp_text_list = []\n",
    "    for idx in range(lines):\n",
    "        if lines[idx].startwith('E'):\n",
    "            tmp_history = []\n",
    "            for item in tmp_text_list[:-1]:\n",
    "                tmp_history.append([word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in item])    \n",
    "            history.append(tmp_history)\n",
    "\n",
    "            true_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_test_list[-1]])\n",
    "\n",
    "            tmp_utt = random_choice_false_response(lines)\n",
    "            false_utt.append([ word2idx[tmp] if tmp in word2idx.keys() else word2idx['_UNK'] for tmp in tmp_utt ])\n",
    "            tmp_text_list = []\n",
    "        elif lines[idx].startwith('M'):\n",
    "            tmp_text_list.append(lines[idx][2:])\n",
    "        else:\n",
    "            print('not start with E or M error')\n",
    "\n",
    "import pickle\n",
    "results = {'history':history, 'true_utt':true_utt, 'false_utt':false_utt}\n",
    "save_file = open(\"results.pkl\",\"wb\")\n",
    "pickle.dump(results, save_file)\n",
    "save_file.close()\n",
    "\n",
    "embedding = {'embedding_matrix':embedding_matrix}\n",
    "save_file = open(\"embedding_matrix.pkl\",\"wb\")\n",
    "pickle.dump(embedding, save_file)\n",
    "save_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#判断是否是汉字\n",
    "def is_all_chinese(strs):\n",
    "    for _char in strs.strip():\n",
    "        if not '\\u4e00' <= _char <= '\\u9fa5':\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def is_chinese(uchar)：\n",
    "    if uchar >= u'\\u4E00' and uchar <= u'\\u9FA5':\n",
    "        return true\n",
    "    else:\n",
    "        return false\n",
    "\n",
    "#判断是否是数字\n",
    "def is_number(uchar)：\n",
    "    if uchar >= u'\\u0030' and uchar <= u'\\u0039':\n",
    "        return true\n",
    "    else:\n",
    "        return false\n",
    "\n",
    "#判断是否是英文字母\n",
    "def is_english(uchar)：\n",
    "    if ( uchar >= u'\\u0041' and uchar <= u'\\u005A' ) or ( uchar >= u'\\u0061' and uchar <= u'\\u007A'):\n",
    "        return true\n",
    "    else:\n",
    "        return false"
   ]
  }
 ]
}