{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.6.0-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python36064bit27be6fbe43c041f6a1f58b4ba6c110f6",
   "display_name": "Python 3.6.0 64-bit"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import pyltp\n",
    "from snownlp import SnowNLP\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 引入模型文件\n",
    "LTP_DATA_DIR = '/Users/sunhongchao/Documents/craft/Awesome/Zero-Preprocessing/resources/ltp_data_v3.4.0'  # ltp模型目录的路径\n",
    "\n",
    "cws_model_path = os.path.join(LTP_DATA_DIR, 'cws.model')  # 分词模型路径，模型名称为`cws.model`\n",
    "pos_model_path = os.path.join(LTP_DATA_DIR, 'pos.model')  # 词性标注模型路径，模型名称为`pos.model`\n",
    "ner_model_path = os.path.join(LTP_DATA_DIR, 'ner.model')  # 命名实体识别模型路径，模型名称为`ner.model`\n",
    "par_model_path = os.path.join(LTP_DATA_DIR, 'parser.model')  # 分词模型路径， 模型名称为'parser.model'\n",
    "\n",
    "\n",
    "#分词\n",
    "from pyltp import Segmentor\n",
    "segmentor = Segmentor()  # 初始化实例\n",
    "segmentor.load(cws_model_path)  # 加载模型\n",
    "\n",
    "#词性标注\n",
    "from pyltp import Postagger\n",
    "postagger = Postagger()  # 初始化实例\n",
    "postagger.load(pos_model_path)  # 加载模型\n",
    "\n",
    "#命名实体识别\n",
    "from pyltp import NamedEntityRecognizer\n",
    "recognizer = NamedEntityRecognizer()  # 初始化实例\n",
    "recognizer.load(ner_model_path)  # 加载模型\n",
    "\n",
    "#句法分析\n",
    "from pyltp import Parser\n",
    "parser = Parser()   # 初始化实例\n",
    "parser.load(par_model_path)   # 加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deal_ner(words_list, postags_list, netags_list, ner_type):\n",
    "\n",
    "    if ner_type == 'name':\n",
    "        _tag = 'Nh'\n",
    "    elif ner_type == 'org':\n",
    "        _tag = 'Ni'\n",
    "    elif ner_type == 'loc':\n",
    "        _tag = 'Nt'\n",
    "    elif ner_type == 'date':\n",
    "        _tag = 'Ns'\n",
    "    else:\n",
    "        pass\n",
    "    print(ner_type)\n",
    "\n",
    "    #去除非命名实体\n",
    "    a = len(words_list)\n",
    "    words_list_1=[]\n",
    "    postags_list_1=[]\n",
    "    netags_list_1=[]\n",
    "    i = 0\n",
    "    while i < a:\n",
    "        if netags_list[i] != 'O':\n",
    "            words_list_1.append(words_list[i])\n",
    "            postags_list_1.append(postags_list[i])\n",
    "            netags_list_1.append(netags_list[i])\n",
    "        i += 1\n",
    "    a1 = len(words_list_1)\n",
    "\n",
    "    #提取人名\n",
    "    i = 0\n",
    "    lists = []\n",
    "    while i<a1:\n",
    "        # 人名\n",
    "        print(i)\n",
    "        if netags_list_1[i] == 'S-'+_tag:\n",
    "            lists.append(words_list_1[i])\n",
    "        elif netags_list_1[i] == 'B-'+ _tag:\n",
    "            temp_s3 = ''\n",
    "            temp_s3 += words_list_1[i]\n",
    "            j = i+1\n",
    "            while (j<a1) and (netags_list_1[j]=='I-'+_tag or netags_list_1[j]=='E-'+_tag):\n",
    "                temp_s3 += words_list_1[j]\n",
    "                j += 1\n",
    "            lists.append(temp_s3)\n",
    "\n",
    "        i += 1\n",
    "\n",
    "    return lists\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_deal(input_str:str):\n",
    "\n",
    "    words = segmentor.segment(input_str)  # 分词\n",
    "    words_list = list(words)   #words_list列表保存着分词的结果\n",
    "    print('word list', words_list)\n",
    "\n",
    "    postags = postagger.postag(words)  # 词性标注\n",
    "    postags_list = list(postags)  #postags_list保存着词性标注的结果\n",
    "    print('pos list', postags_list)\n",
    "\n",
    "    netags = recognizer.recognize(words, postags)  # 命名实体识别\n",
    "    netags_list = list(netags)  #netags_list保存着命名实体识别的结果\n",
    "    print('ner list', netags_list)\n",
    "\n",
    "    # words = ['元芳', '你', '怎么', '看']\n",
    "    # postags = ['nh', 'r', 'r', 'v']\n",
    "    arcs = parser.parse(words, postags)   # 句法分析\n",
    "    print('\\t'.join('%d: %s' %(arc.head, arc.relation) for arc in arcs))\n",
    "\n",
    "    # # 关键词\n",
    "    # from snownlp import SnowNLP\n",
    "\n",
    "    # s = SnowNLP(input_str)\n",
    "    # keywords_list = s.keywords(5)\n",
    "\n",
    "    # # 长句压缩\n",
    "    # if len(input_str) > 20:\n",
    "\n",
    "    \n",
    "    # # 信息量衡量\n",
    "\n",
    "    # # QQ相似， 找到类似的Q\n",
    "         \n",
    "\n",
    "    return words_list, postags_list, netags_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = query_deal('姚明在08年北京奥运会上担任旗手')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ltp_release():\n",
    "    segmentor.release()  # 释放模型\n",
    "    postagger.release()  # 释放模型\n",
    "    recognizer.release()  # 释放模型\n",
    "    parser.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "['领域', '智能', '人工', '科学', '计算机']\n"
    }
   ],
   "source": [
    "# 关键词\n",
    "from snownlp import SnowNLP\n",
    "\n",
    "s = SnowNLP('自然语言处理是计算机科学领域与人工智能领域中的一个重要方向')\n",
    "s_key = s.keywords(5)\n",
    "print(s_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}